# Word Embeddings

## What are word embeddings ?

Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually it involves a mathematical embedding from a space with one dimension per word to a continuous vector space with much lower dimension.

## About Repo

This repository contains scripts for playing around with the common pretrained work embedding vectors like `Fasttext` , `Glove`

